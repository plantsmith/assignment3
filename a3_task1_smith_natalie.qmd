---
title: "Palmetto binary logistic regression (individual)"
author: "Natalie Smith"
format: 
  html:
    embed-resources: true
    code-fold: true
    toc: true
execute:
  warning: false
  message: false
---

```{r}
library(tidyverse) 
library(here)
library(tidymodels) 
library(ggplot2)
```


```{r}
#pull in the data
p_df <- read.csv(here("data/palmetto.csv"))

#clean data
palmetto <- p_df %>%
  select(species,height,length,width,green_lvs) %>% 
  mutate(species=as_factor(species)) %>% 
  drop_na()
```

```{r}
#explatory graphs: visualizations (with figure captions) in  height, canopy length, canopy width, and green leaves for the two species. If you prefer, combine the figures into a compound figure using patchwork or cowplot. Below your data visualizations, add a sentence or two with a takeaway from the plots, e.g., based on these plots, which predictor variables are more likely to help classify species correctly?

ggplot(palmetto, aes(x = as.factor(species), y = green_lvs)) +
  geom_boxplot() +
  scale_x_discrete(labels = c("1" = "S.repens", "2" = "S.etonia")) +
  labs(x = " ")


ggplot(palmetto, aes(x= as_factor(species), y = height))+
  geom_boxplot()

ggplot(palmetto, aes(x= as_factor(species), y = length))+
  geom_boxplot()

ggplot(palmetto, aes(x= as_factor(species), y = width))+
  geom_boxplot()

```
1 = S. repens
2 = S. etonia

```{r}
# Define formulas:
f1 <- species ~ height + length + width + green_lvs
f2 <- species ~ height + width + green_lvs

# BLR:
blr1 <- glm(formula = f1, data = palmetto, family = binomial)
blr2 <- glm(formula = f2, data = palmetto, family = binomial)

# Summarize:
summary(blr1)
summary(blr2)

```

COEF tell us LOG ODDS: 

Thoughts: 
F1 - all p values are teeny, v important
Coefficients:
             Estimate Std. Error z value Pr(>|z|)    
(Intercept)  3.226685   0.142071   22.71   <2e-16 ***
height      -0.029217   0.002306  -12.67   <2e-16 ***
length       0.045823   0.001866   24.56   <2e-16 ***
width        0.039443   0.002100   18.78   <2e-16 ***
green_lvs   -1.908475   0.038863  -49.11   <2e-16 ***

F2 - 
Coefficients:
             Estimate Std. Error z value Pr(>|z|)    
(Intercept)  3.767722   0.135314  27.844   <2e-16 ***
height      -0.002950   0.001881  -1.568    0.117    
width        0.069019   0.001785  38.661   <2e-16 ***
green_lvs   -1.884815   0.036127 -52.172   <2e-16 ***

green leaves has largest effect on log odds. wtf is going on w height - tbd

```{r}
#Check the split:
palmetto %>%
  group_by(species) %>%
  summarize(n = n()) %>%
  ungroup() %>%
  mutate(prop = n / sum(n))

set.seed(123)

#Spilt into training and testing:
p_split <- initial_split(palmetto, prop = 0.80, strata = species) 

# Create Dataframes:
p_train_df <- training(p_split) 
p_test_df <- testing(p_split)

```

# if i set up a workflow below, do I have to do this step?
```{r}
#binary logistic regression model with our data

#this is the default function:
blr_mdl <- logistic_reg() %>%
  set_engine('glm')

#f1
blr_fit1 <- blr_mdl %>%
  fit(formula = f1, data = p_train_df)

#f2
blr_fit2<- blr_mdl %>%
  fit(formula = f2, data =  p_train_df)

blr_fit1
blr_fit2
```
First model is better - lower AIC

```{r}
# use our fitted model from the training set on the test set #1
p_test_predict1 <- p_test_df %>%
  mutate(predict(blr_fit1, new_data = p_test_df)) %>%
  mutate(predict(blr_fit1, new_data = ., type = 'prob'))

# use our fitted model from the training set on the test set #2
p_test_predict2 <- p_test_df %>%
  mutate(predict(blr_fit2, new_data = p_test_df)) %>%
  mutate(predict(blr_fit2, new_data = ., type = 'prob'))

```

```{r}
#accuracy #1
accuracy(p_test_predict1, truth = species, estimate = .pred_class)

#accuracy #2
accuracy(p_test_predict2, truth = species, estimate = .pred_class)

```

(1)
accuracy	binary	0.9123879	
(2)
accuracy	binary	0.896903	

```{r}
#ROC
roc_df <- roc_curve(p_test_predict1, truth = species, .pred_0)
autoplot(roc_df)
```


```{r}
### Calculate area under curve - 50% is random guessing, 100% is perfect classifier
yardstick::roc_auc(p_test_predict1, truth = species, .pred_0)
yardstick::roc_auc(p_test_predict2, truth = species, .pred_0)
```


```{r}
#FOLDs: 
set.seed(10101)
p_train_folds <- vfold_cv(p_train_df, v = 10)
p_train_folds
```

```{r}
#workflow
blr_wf1 <- workflow() %>%  
  add_model(blr_mdl) %>%
  add_formula(f1)

blr_wf2 <- workflow() %>%  
  add_model(blr_mdl) %>%
  add_formula(f2)
 
```
QUESTION:
#what is the difference between regular accuracy and ROC?

```{r}
#apply to folded datasetls: 
blr_fit_folds1 <- blr_wf1 %>%
  fit_resamples(p_train_folds)

blr_fit_folds2 <- blr_wf2 %>%
  fit_resamples(p_train_folds)

blr_fit_folds1
blr_fit_folds2

# Metrics:
collect_metrics(blr_fit_folds1)

#accuracy: mean 0.9174543	n 10	std: 0.0025763289
#roc: mean 0.9732677	n 10	std: 0.0008184978 
collect_metrics(blr_fit_folds2)
#accuracy: mean 0.8997234	n 10	std 0.003419435
#roc: mean 0.9638196	n 10	std 0.001191673

#ROC close to 100 means, close to a perfect classifier. 
```
Model 1:
accuracy	binary	0.9174543	10	0.0025763289	
roc_auc	binary	0.9732677	10	0.0008184978	

Model 2:

accuracy	binary	0.8997234	10	0.003419435	
roc_auc	binary	0.9638196	10	0.001191673

Based on AIC and ROC - we are going with Model 1 (lower AIC, highter ROC)

Now, we train it: Train your selected model using the entire dataset, and create a finalized table (e.g., knitr::kable() and kableExtra functions) containing the binary logistic regression model results (at least coefficients, standard errors for the coefficients, and information for significance - consider using broom::tidy() to get you most of the way). 

```{r}
#Run Model 1 on entire data set: 
set.seed(10101)
p_folds <- vfold_cv(palmetto, v = 10)
p_folds

blr_wf1 <- workflow() %>%  
  add_model(blr_mdl) %>%
  add_formula(f1)

blr_fit_folds <- blr_wf1 %>%
  fit_resamples(p_folds)
```



```{r}
# Calculate probabilities for each plant in the original dataset
predicted_probabilities <- predict(final_model, new_data = original_dataset, type = "prob")

# Extract probabilities for each species
species_probabilities <- predicted_probabilities %>%
  as.data.frame() %>%
  mutate(predicted_species = ifelse(.pred_class == "SpeciesA", 1, 0)) # Assuming 1 represents SpeciesA and 0 represents SpeciesB

# Add a column for predicted species based on 50% cutoff
species_probabilities <- species_probabilities %>%
  mutate(predicted_species = ifelse(.pred_class >= 0.5, "SpeciesA", "SpeciesB")) # Change 0.5 to any desired cutoff

# Combine with original dataset
combined_data <- cbind(original_dataset, species_probabilities)

# Calculate number of plants correctly and incorrectly classified
classification_results <- combined_data %>%
  group_by(species, predicted_species) %>%
  summarize(count = n()) %>%
  ungroup() %>%
  mutate(correctly_classified = ifelse(species == predicted_species, count, 0),
         incorrectly_classified = ifelse(species != predicted_species, count, 0))

# Calculate percentage correctly classified for each species
classification_results <- classification_results %>%
  group_by(species) %>%
  summarize(total_count = sum(count),
            total_correct = sum(correctly_classified),
            percent_correct = (total_correct / total_count) * 100)

# Create a finalized table
final_table <- classification_results %>%
  mutate(species = factor(species),
         percent_correct = paste0(round(percent_correct, 2), "%")) %>%
  kable(caption = "Classification Results Based on Logistic Regression Model",
        col.names = c("Species", "Total Count", "Correctly Classified", "Incorrectly Classified", "% Correctly Classified"))

```









```{r}
# Create a finalized table
final_table <- kable(model_results, align = "c") %>%
  kable_styling() %>%
  add_header_above(c(" ", "Estimate", "Std. Error", "z value", "Pr(>|z|)"))

```

